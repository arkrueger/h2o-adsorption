import numpy as np
import scipy as sc
import scipy.optimize
from time import sleep
import matplotlib
import matplotlib.pyplot as plt

'''
function find_mean_Z
This function finds all data points at the desired saturation (+/- a specified tolerance) at each time "slice" 
and finds the average of the corresponding Z-values. In other words, Z-values are a measure of how the saturation 
progresses up the column with time. Recall that each time "slice" is an image of the color-changing 
silica column which produces a data set relating saturation and Z-position. If the saturation of interest  
is 50%, this function returns a 1D array (length: number of time points of given trial) containing a series of Z-values 
that express how far the line of 50% saturation has progressed along the column at each time point.

Big picture: 
This function is useful for two purposes. The first is to fit the parameter psi at saturation = 50% (at 
50% saturation the governing equations simplify such that psi is the slope of the Z vs. tau trendline). For further 
explanation see the ReadMe. The second purpose is to fit the parameter kappa by nonlinear regression. To do this 
directly with the S matrix (i.e. generate an S matrix from a guessed kappa value, compute a score of closeness between 
the two S matrices, and attempt to minimize this score by guessing new kappas) would be both needlessly complex and 
susceptible to over-fitting. Instead, the Z-values at three representative saturation levels are used. 
The Z-values are a useful measure of the overall trend of the data set. This is useful because it gives us a way to 
reduce the complexity of the data, as the S matrix has dimensions [# of time points]x[# of Z points]. In this analysis, 
we will look at the data set generated by find_mean_Z for 3 saturation levels. The specific points can depend on the 
quality and spread of the data, but typically 35%, 50%, and 65% saturation work well to capture important inflection 
points for nonlinear regression. By reducing this large matrix to 3 much smaller data sets, much less time and 
computation power is required to fit the parameter kappa.

Visualization:
The first set of graphs (count: number of time points of given trial) show the Z-values selected for averaging
at each time point. The second set (count: one) shows all points on axes Z vs. tau. 
'''


def find_mean_Z(trial, saturation, show_graphs=0):
    # trial is the dict corresponding to a single trial
    # saturation is the value of S at which the value of Z is averaged
    # show_graphs toggles the graphing feature (1 means show, 0 means hide)

    # Returns:
    # mean_Z, An array containing a series of Z-values (length: number of time points of given trial)
    # fit, an array containing the polynomial coefficients from the (linear) fit made to the Z vs. tau plot

    # We're not editing the S_matrix, so we can give it a new name for code readability.
    S_matrix = trial['S_matrix']
    tolerance = 0.01  # No values will be exactly at the specified saturation; this is the +/- tolerance range
    # For each row (time step) find the indices whose elements satisfy the condition of being close to the specified
    # saturation level and take the mean of those indices. Divide by the length of that dimension to normalize.
    # Note: we aren't interested in the saturation values directly here, but the Z values. That's why the index is
    # used, because the x2 index represents z-position (technically, pixels).
    mean_Z = np.array([np.mean(np.argwhere(abs(row - saturation) < tolerance)) for row in S_matrix]) / S_matrix.shape[1]
    std_Z = np.array([np.std(np.argwhere(abs(row - saturation) < tolerance)) for row in S_matrix]) / S_matrix.shape[1]
    # Leave the NaN values in there. Important to preserve spacing because the index is the time axis.

    if show_graphs:
        time_point = 0.0
        x_array = np.linspace(0, 100, S_matrix.shape[1])  # % column height on the x axis
        fig = plt.figure()
        plt.axis([0, 100, 0, 100])
        plt.xlabel('Z (% column height)', size=20)
        plt.ylabel('S (% saturation)', size=20)
        # Initialize plot lines with meaningless data that is not displayed.
        init_ydata = np.full(x_array.shape[0], -1)
        all_points_line, = plt.plot(x_array, init_ydata, '.', label='All Data', color='b')
        selected_points_line, = plt.plot(x_array, init_ydata, '.', label='Selected Points', color='r')
        mean_Z_line = plt.axvline(-1, 0, 100, label='Mean Z')  # This will be the vertical line marking mean_Z.
        # Recall mean_Z is a scalar, the vertical line is just for visualization.
        plt.figlegend(prop={'size': 20})
        fig.axes[0].tick_params(labelsize=20)
        # Highlight the boundary of selected values.
        plt.axhspan((saturation - tolerance) * 100, (saturation + tolerance) * 100, color='b', alpha=0.5)
        # Set window size and make the figure window pop up in front.
        fig.set_size_inches(20, 15)
        window = plt.get_current_fig_manager().window
        window.activateWindow()
        window.raise_()
        # The comma after each respective "line" above is necessary because plt.plot returns a list.
        for each in S_matrix[:]:  # Iterate over S matrix rows
            row = np.copy(each)

            # Record the points that fall within tolerance of desired saturation.
            selected_points_idx = np.argwhere(abs(row - saturation) < tolerance)
            selected_points_x = x_array[selected_points_idx]
            selected_points_y = row[selected_points_idx] * 100
            # Average Z for this time step (this is a scalar).
            row_wise_Z = np.mean(selected_points_x)

            row *= 100  # The y-axis is percent saturation so we need to scale the values before plotting.

            # Constructing the graph.
            all_points_line.set_ydata(row)
            # Need to set both y AND x data for selected points because the shape of the data changes each iteration.
            selected_points_line.set_xdata(selected_points_x)
            selected_points_line.set_ydata(selected_points_y)
            mean_Z_line.set_xdata(row_wise_Z)  # Draw vertical line corresponding to mean_Z.
            plt.title(trial['trial_name'] + '\nTime = ' + str(time_point) + ' minutes', size=20)
            plt.axis([0, 100, 0, 100])  # Necessary in case the user closed the window mid-execution.

            plt.pause(0.25)

            time_point += 0.25

        plt.pause(0.75)  # Pause a bit longer before closing the figure.
        plt.close()

        # Show all Z points vs time.
        mean_Z_scaled = mean_Z * 100
        std_Z_scaled = std_Z * 100
        x_array_summary = np.linspace(0, (S_matrix.shape[0] - 1) / 4, S_matrix.shape[0])
        fig_summary = plt.figure()
        plt.axis([-0.25, 0.25 + x_array_summary[-1], 0, 100])  # Keep xlim bounds to dataset range + 1.
        plt.xlabel('Time (minutes)', size=20)
        plt.ylabel('Z (% column height)', size=20)
        plt.title('Mean Z Over Time', size=20)
        # plt.plot(x_array_summary, mean_Z_scaled, 'o', label='Mean Z', color='b')
        plt.errorbar(x_array_summary, mean_Z_scaled, yerr=std_Z_scaled, marker='o', linestyle='', color='b',
                     label='Mean Z', capsize=5)
        # Recall mean_Z is a scalar, the vertical line is just for visualization.
        plt.figlegend(prop={'size': 20})
        fig_summary.axes[0].tick_params(labelsize=20)
        # Set window size and make the figure window pop up in front.
        fig_summary.set_size_inches(20, 15)
        window = plt.get_current_fig_manager().window
        window.activateWindow()
        window.raise_()

        plt.pause(3)
        plt.close()

    return mean_Z


'''
function finite_difference
This function...

Big picture:
This function...

Visualization:
None
'''


def finite_difference(trial, kappa):
    # Note: The kappa parameter and the kappa attribute inside the trial dict are not necessarily (and are often not)
    # the same value. The kappa parameter on its own is a "guess" typically supplied by the nonlinear regression
    # function, and varies until the regression converges on the final kappa value. The kappa attribute in the trial
    # dict is assigned this final kappa value after the regression is complete. Within the finite_difference function,
    # the attribute is NOT used.

    # Finite Difference Algorithm
    # The initial and boundary conditions require a finite difference scheme as follows:
    # Forward difference with respect to S (saturation)
    # Backward difference with respect to tau (time)
    # Central difference with respect to Z (position)

    # For numerical stability, the finite difference method needs smaller time and position steps than the sampling
    # rate of the actual experiment.
    num_of_frames = len(trial['S_matrix'][:, 0])  # number of images collected in the actual experiment

    time_step = 1.0  # seconds - this is the "real" time increment used for the finite difference scheme
    tau_step = time_step * trial['tau_factor']
    Z_step = 0.01  # Nothing fancy went on to arrive at this number - it's simply small enough for numerical stability

    psi = trial['psi']

    tau_dim = (num_of_frames - 1) * 15 + 1
    Z_dim = int(1 / Z_step)
    # Initialize the matrix to be populated by the finite difference scheme.
    S_matrix_fd = np.zeros((tau_dim, Z_dim))
    G_matrix_fd = np.copy(S_matrix_fd)

    G_matrix_fd[:, 0] = 1
    for k in range(Z_dim):
        for i in range(1, tau_dim):
            # Finite difference index mappings. Not really necessary except for i_S but renaming them helps keep the
            # math clear.
            i_S = i - 1  # forward difference, unknown is ahead of the local i-index
            k_S = k
            i_G = i
            k_G = k

            S_matrix_fd[i_S + 1, k_S] = S_matrix_fd[i_S, k_S] + tau_step * psi * kappa * (
                    G_matrix_fd[i_S, k_S] - S_matrix_fd[i_S, k_S])
            # Fill the G matrix starting with the second column because the first column is solid 1's according to the
            # boundary conditions.
            if k >= 1:
                G_matrix_fd[i_G, k_G] = (1 / (tau_step + Z_step)) * (
                        Z_step * G_matrix_fd[i_G - 1, k_G] + tau_step * G_matrix_fd[i_G, k_G - 1] - tau_step * Z_step *
                        kappa * (G_matrix_fd[i_G - 1, k_G] - S_matrix_fd[i_G - 1, k_G]))

    # (The G matrix is not used for anything except the construction of the S matrix by finite difference.)
    # Decimate the S_matrix to the time sample rate of the original experiment. We only care about the time points
    # so it's okay that the resolution in the Z-dimension is different between the experimental and finite difference
    # matrices.
    S_matrix_fd = S_matrix_fd[0:tau_dim:15, :]
    return S_matrix_fd


'''
function kappa_residuals
This function...

Big picture:
This function...

Visualization:
None
'''


def fit_kappa(trial, kappa_init_guess, show_graphs):
    # To do the nonlinear regression, we pass a function to scipy.optimize.fmin and ask it to minimize the output.
    # calculate_sum_of_squared_residuals computes a numerical score that tells how well the generated S matrix fits the
    # experimental S matrix by comparing the Z values at three saturation levels (35%, 50%, and 65%).
    # Compute regression score (sum of squared residuals).
    def calculate_sum_of_squared_residuals(trial, kappa_guess, show_graphs):
        # Set up a dict for the generated data. The "guess" refers to the fact that the nonlinear regression algorithm
        # makes a series of guesses while searching for the minimum.
        trial_guess = {}
        # Using finite_difference, generate an S matrix based on the provided kappa.
        trial_guess['S_matrix'] = finite_difference(trial, kappa_guess)  # NOT trial_guess
        trial_guess['Z_35'] = find_mean_Z(trial_guess, 0.35, 0)
        trial_guess['Z_50'] = find_mean_Z(trial_guess, 0.50, 0)
        trial_guess['Z_65'] = find_mean_Z(trial_guess, 0.65, 0)

        # Double checking that array lengths match, just in case.
        if not np.array_equal([len(trial['Z_35']), len(trial['Z_50']), len(trial['Z_65'])], [
                len(trial_guess['Z_35']), len(trial_guess['Z_50']), len(trial_guess['Z_65'])]):
            print('One or more Z_sat dimensions do not agree between generated and experimental Z_sat arrays.')

        sum_of_squared_residuals = 0
        for Z_sat in ['Z_35', 'Z_50', 'Z_65']:
            temp_Z = np.copy(trial[Z_sat])
            temp_Z_guess = np.copy(trial_guess[Z_sat])
            # NaN values in the Z_sat array do not affect the regression score, which means that an S matrix yielding
            # a Z_sat array containing only NaN values will ordinarily produce the lowest (most desirable) score.
            # However, this would lead the nonlinear regression algorithm to produce a nonsensical, unoptimized kappa
            # value. In order to prevent this from happening, the NaN values in the experimental Z_sat arrays are
            # set to zero, as are the corresponding elements of the generated (guess) Z_sat.
            # The result of this is that at each respective saturation level (Z_35, Z_50, or Z_65) time points are
            # ignored if the experimental Z_sat data contains an NaN value at that time point. If the generated Z_sat
            # data contains NaN value at any given time point, the SSR score at that time point is computed as if the
            # generated Z_sat value was in fact zero.
            # In addition, NaN values produced in the experimental Z_sat cannot be fixed (can't supply data that isn't
            # there) so values at these positions are converted to zero in BOTH experimental and generated data.
            temp_Z[~np.isfinite(temp_Z)] = 0  # Convert experimental NaN values to zero.
            # Do the same for the corresponding elements in Z_guess, plus convert all Z_guess NaNs to zero.
            temp_Z_guess[np.bitwise_or(~np.isfinite(trial[Z_sat]), ~np.isfinite(trial_guess[Z_sat]))] = 0
            # Square the difference between corresponding elements and compute the sum.
            sum_of_squared_residuals += sum(np.square(temp_Z - temp_Z_guess))

        return sum_of_squared_residuals

    # Find the kappa by nonlinear regression.
    f = lambda k: calculate_sum_of_squared_residuals(trial, k, show_graphs)
    kappa_fit = sc.optimize.fmin(func=f, x0=kappa_init_guess)

    return kappa_fit
