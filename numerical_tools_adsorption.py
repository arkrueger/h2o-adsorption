import numpy as np
import scipy as sc
import scipy.optimize

'''
function find_mean_Z
This function finds all data points at the desired saturation (+/- a specified tolerance) at each time "slice" 
and finds the average of the corresponding Z-values. In other words, Z-values are a measure of how the saturation 
progresses up the column with time. Recall that each time "slice" is an image of the color-changing 
silica column which produces a data set relating saturation and Z-position. If the saturation of interest  
is 50%, this function returns a 1D array (length: number of time points of given trial) containing a series of Z-values 
that express how far the line of 50% saturation has progressed along the column at each time point.

Big picture: 
This function is useful for two purposes. The first is to fit the parameter psi at saturation = 50% (at 
50% saturation the governing equations simplify such that psi is the slope of the Z vs. tau trendline). For further 
explanation see the ReadMe. The second purpose is to fit the parameter kappa by nonlinear regression. To do this 
directly with the S matrix (i.e. generate an S matrix from a guessed kappa value, compute a score of closeness between 
the two S matrices, and attempt to minimize this score by guessing new kappas) would be both needlessly complex and 
susceptible to over-fitting. Instead, the Z-values at three representative saturation levels are used. 
The Z-values are a useful measure of the overall trend of the data set. This is useful because it gives us a way to 
reduce the complexity of the data, as the S matrix has dimensions [# of time points]x[# of Z points]. In this analysis, 
we will look at the data set generated by find_mean_Z for 3 saturation levels. The specific points can depend on the 
quality and spread of the data, but typically 35%, 50%, and 65% saturation work well to capture important inflection 
points for nonlinear regression. By reducing this large matrix to 3 much smaller data sets, much less time and 
computation power is required to fit the parameter kappa.

Visualization:
The first set of graphs (count: number of time points of given trial) show the Z-values selected for averaging
at each time point. The second set (count: one) shows all points on axes Z vs. tau. 
'''


def find_mean_Z(trial, saturation, show_graphs=0):
    # trial is the dict corresponding to a single trial
    # saturation is the value of S at which the value of Z is averaged
    # show_graphs toggles the graphing feature (1 means show, 0 means hide)

    # Returns:
    # mean_Z, An array containing a series of Z-values (length: number of time points of given trial)
    # fit, an array containing the polynomial coefficients from the (linear) fit made to the Z vs. tau plot

    # We're not editing the S_matrix, so we can give it a new name for code readability.
    S_matrix = trial['S_matrix']
    tolerance = 0.01  # No values will be exactly at the specified saturation; this is the +/- tolerance range
    # For each row (time step) find the indices whose elements satisfy the condition of being close to the specified
    # saturation level and take the mean of those indices. Divide by the length of that dimension to normalize.
    # Note: we aren't interested in the saturation values directly here, but the Z values. That's why the index is
    # used, because the x2 index represents z-position (technically, pixels).
    mean_Z = np.array([np.mean(np.argwhere(abs(row - saturation) < tolerance)) for row in S_matrix]) / S_matrix.shape[1]
    # Leave the NaN values in there. Important to preserve spacing because the index is the time axis.

    # TODO graphing component
    # For the graphing, keep the non-dimensional scaled Z
    # BUT just use frames or minutes for the time axis because it communicates what is happening more clearly than
    # tau does
    # if show_graphs == 1:
    # print('test')
    # do graphing

    # TODO consider having an if statement that toggles between the long way to get mean_Z and the concise one
    #  depending if show_graphs is selected

    return mean_Z


'''
function finite_difference
This function...

Big picture:
This function...

Visualization:
None
'''


def finite_difference(trial, kappa):
    # Note: The kappa parameter and the kappa attribute inside the trial dict are not necessarily (and are often not)
    # the same value. The kappa parameter on its own is a "guess" typically supplied by the nonlinear regression
    # function, and varies until the regression converges on the final kappa value. The kappa attribute in the trial
    # dict is assigned this final kappa value after the regression is complete. Within the finite_difference function,
    # the attribute is NOT used.

    # Finite Difference Algorithm
    # The initial and boundary conditions require a finite difference scheme as follows:
    # Forward difference with respect to S (saturation)
    # Backward difference with respect to tau (time)
    # Central difference with respect to Z (position)

    # For numerical stability, the finite difference method needs smaller time and position steps than the sampling
    # rate of the actual experiment.
    num_of_frames = len(trial['S_matrix'][:, 0])  # number of images collected in the actual experiment

    time_step = 1.0  # seconds - this is the "real" time increment used for the finite difference scheme
    tau_step = time_step * trial['tau_factor']
    Z_step = 0.01  # Nothing fancy went on to arrive at this number - it's simply small enough for numerical stability

    psi = trial['psi']

    tau_dim = (num_of_frames - 1) * 15 + 1
    Z_dim = int(1 / Z_step)
    # Initialize the matrix to be populated by the finite difference scheme.
    S_matrix_fd = np.zeros((tau_dim, Z_dim))
    G_matrix_fd = np.copy(S_matrix_fd)

    G_matrix_fd[:, 0] = 1
    for k in range(Z_dim):
        for i in range(1, tau_dim):
            # Finite difference index mappings. Not really necessary except for i_S but renaming them helps keep the
            # math clear.
            i_S = i - 1  # forward difference, unknown is ahead of the local i-index
            k_S = k
            i_G = i
            k_G = k

            S_matrix_fd[i_S + 1, k_S] = S_matrix_fd[i_S, k_S] + tau_step * psi * kappa * (
                    G_matrix_fd[i_S, k_S] - S_matrix_fd[i_S, k_S])
            # Fill the G matrix starting with the second column because the first column is solid 1's according to the
            # boundary conditions.
            if k >= 1:
                G_matrix_fd[i_G, k_G] = (1 / (tau_step + Z_step)) * (
                        Z_step * G_matrix_fd[i_G - 1, k_G] + tau_step * G_matrix_fd[i_G, k_G - 1] - tau_step * Z_step *
                        kappa * (G_matrix_fd[i_G - 1, k_G] - S_matrix_fd[i_G - 1, k_G]))

    # (The G matrix is not used for anything except the construction of the S matrix by finite difference.)
    # Decimate the S_matrix to the time sample rate of the original experiment. We only care about the time points
    # so it's okay that the resolution in the Z-dimension is different between the experimental and finite difference
    # matrices.
    S_matrix_fd = S_matrix_fd[0:tau_dim:15, :]
    return S_matrix_fd


'''
function kappa_residuals
This function...

Big picture:
This function...

Visualization:
None
'''


def fit_kappa(trial, kappa_init_guess, show_graphs):
    import itertools

    # To do the nonlinear regression, we pass a function to scipy.optimize.fmin and ask it to minimize the output.
    # calculate_sum_of_squared_residuals computes a numerical score that tells how well the generated S matrix fits the
    # experimental S matrix by comparing the Z values at three saturation levels (35%, 50%, and 65%).
    # Compute regression score (sum of squared residuals).
    def calculate_sum_of_squared_residuals(trial, kappa_guess, show_graphs):
        # Set up a dict for the generated data. The "guess" refers to the fact that the nonlinear regression algorithm
        # makes a series of guesses while searching for the minimum.
        trial_guess = {}
        # Using finite_difference, generate an S matrix based on the provided kappa.
        trial_guess['S_matrix'] = finite_difference(trial, kappa_guess)  # NOT trial_guess
        trial_guess['Z_35'] = find_mean_Z(trial_guess, 0.35, 0)
        trial_guess['Z_50'] = find_mean_Z(trial_guess, 0.50, 0)
        trial_guess['Z_65'] = find_mean_Z(trial_guess, 0.65, 0)

        # Double checking that array lengths match, just in case.
        if not np.array_equal([len(trial['Z_35']), len(trial['Z_50']), len(trial['Z_65'])], [
                len(trial_guess['Z_35']), len(trial_guess['Z_50']), len(trial_guess['Z_65'])]):
            print('One or more Z_sat dimensions do not agree between generated and experimental Z_sat arrays.')

        sum_of_squared_residuals = 0
        for Z_sat in ['Z_35', 'Z_50', 'Z_65']:
            temp_Z = np.copy(trial[Z_sat])
            temp_Z_guess = np.copy(trial_guess[Z_sat])
            # NaN values in the Z_sat array do not affect the regression score, which means that an S matrix yielding
            # a Z_sat array containing only NaN values will ordinarily produce the lowest (most desirable) score.
            # However, this would lead the nonlinear regression algorithm to produce a nonsensical, unoptimized kappa
            # value. In order to prevent this from happening, the NaN values in the experimental Z_sat arrays are
            # set to zero, as are the corresponding elements of the generated (guess) Z_sat.
            # The result of this is that at each respective saturation level (Z_35, Z_50, or Z_65) time points are
            # ignored if the experimental Z_sat data contains an NaN value at that time point. If the generated Z_sat
            # data contains NaN value at any given time point, the SSR score at that time point is computed as if the
            # generated Z_sat value was in fact zero.
            # In addition, NaN values produced in the experimental Z_sat cannot be fixed (can't supply data that isn't
            # there) so values at these positions are converted to zero in BOTH experimental and generated data.
            temp_Z[~np.isfinite(temp_Z)] = 0  # Convert experimental NaN values to zero.
            # Do the same for the corresponding elements in Z_guess, plus convert all Z_guess NaNs to zero.
            temp_Z_guess[np.bitwise_or(~np.isfinite(trial[Z_sat]), ~np.isfinite(trial_guess[Z_sat]))] = 0
            # Square the difference between corresponding elements and compute the sum.
            sum_of_squared_residuals += sum(np.square(temp_Z - temp_Z_guess))

        # for Z_sat, Z_sat_guess in itertools.zip([trial['Z_35'], trial['Z_50'], trial['Z_65']], [trial_guess['Z_35_guess'], trial_guess['Z_50_guess'], trial_guess['Z_65_guess']]):
        #     for exp, guess in itertools.zip(Z_sat, Z_sat_guess):

        # sum_of_squared_residuals = 1  # finite_difference(trial, kappa_guess)
        return sum_of_squared_residuals

    # Find the kappa by nonlinear regression.
    f = lambda k: calculate_sum_of_squared_residuals(trial, k, show_graphs)
    # def f(k, *args):
    #     # args[0] should be the trial dict
    #     # args[1] should be show_graphs
    #     return calculate_sum_of_squared_residuals(args[0], k, args[1])
    kappa_fit = sc.optimize.fmin(func=f, x0=kappa_init_guess)#, args=(trial, show_graphs))
    # kappa_fit = sc.optimize.fmin(func=calculate_sum_of_squared_residuals(trial, k, show_graphs), x0=kappa_init_guess)
    # kappa_fit = calculate_sum_of_squared_residuals(trial, kappa_init_guess, show_graphs)
    # kappa_fit = f(3, trial, 0)
    return kappa_fit
